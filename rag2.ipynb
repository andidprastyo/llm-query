{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andidprastyo\\Documents\\Github\\llm-query\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import requests\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Ollama API endpoint\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def query_llm(prompt: str) -> str:\n",
    "    \"\"\"Query Ollama API directly\"\"\"\n",
    "    payload = {\n",
    "        \"model\": \"llama3.2:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"LLM query failed: {e}\")\n",
    "        return \"Error generating response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_record(record: Dict) -> str:\n",
    "    \"\"\"Convert record to searchable text\"\"\"\n",
    "    formatted = []\n",
    "    for k, v in record.items():\n",
    "        if v:\n",
    "            if isinstance(v, (int, float)):\n",
    "                formatted.append(f\"{k}: {v}\")\n",
    "            elif isinstance(v, bool):\n",
    "                formatted.append(f\"{k}: {'yes' if v else 'no'}\")\n",
    "            elif isinstance(v, str):\n",
    "                v = v.strip().replace('\\n', ' ')\n",
    "                if v:\n",
    "                    formatted.append(f\"{k}: {v}\")\n",
    "    return \" | \".join(formatted)\n",
    "\n",
    "# Load JSONL data\n",
    "with open('./fastapi/app/data/test_data.jsonl', 'r') as f:\n",
    "    raw_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Process documents\n",
    "documents = []\n",
    "document_contents = []\n",
    "for record in raw_data:\n",
    "    content = preprocess_record(record)\n",
    "    documents.append({\"content\": content, \"metadata\": record})\n",
    "    document_contents.append(content)\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = encoder.encode(document_contents)\n",
    "\n",
    "# Initialize FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "print(f\"Processed {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Retrieve relevant documents\"\"\"\n",
    "    try:\n",
    "        query_embedding = encoder.encode([query])\n",
    "        D, I = index.search(query_embedding.astype('float32'), k)\n",
    "        return [documents[idx] for idx in I[0]]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Retrieval failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_response(query: str) -> str:\n",
    "    \"\"\"Generate response using RAG\"\"\"\n",
    "    try:\n",
    "        relevant_docs = retrieve(query)\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            return \"No relevant information found.\"\n",
    "        \n",
    "        # Prepare context\n",
    "        context_parts = []\n",
    "        for doc in relevant_docs:\n",
    "            metadata = doc['metadata']\n",
    "            \n",
    "            # Common fields for all queries\n",
    "            common_fields = [\n",
    "                'id',           # Contract ID\n",
    "                'nama',         # Service Name\n",
    "                'customer',     # Customer Name\n",
    "                'sid',          # Service ID\n",
    "                'sid_tsat',     # TSAT Service ID\n",
    "                'no_wo',        # Work Order Number\n",
    "                'start_kontrak',  # Contract Start\n",
    "                'end_kontrak'   # Contract End\n",
    "            ]\n",
    "            \n",
    "            # Additional fields based on query type\n",
    "            if any(word in query.lower() for word in ['bank', 'banking']):\n",
    "                extra_fields = ['layanan', 'segmen', 'datarate_layanan', 'project_nama']\n",
    "            elif 'vsat' in query.lower():\n",
    "                extra_fields = ['produk', 'layanan', 'datarate_layanan', 'uplink', 'downlink']\n",
    "            else:\n",
    "                extra_fields = ['layanan', 'segmen', 'provinsi', 'kabupaten']\n",
    "            \n",
    "            # Combine fields\n",
    "            all_fields = common_fields + extra_fields\n",
    "            \n",
    "            # Format context with clear sections\n",
    "            context_part = \"SERVICE DETAILS:\\n\" + \"\\n\".join(\n",
    "                f\"{k.replace('_', ' ').title()}: {metadata.get(k, 'N/A')}\"\n",
    "                for k in all_fields \n",
    "                if metadata.get(k)\n",
    "            )\n",
    "            context_parts.append(context_part)\n",
    "        \n",
    "        context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Based on the following service and contract records:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions:\n",
    "1. Focus on the relevant contract and service information from the context\n",
    "2. Include specific details like:\n",
    "   - Contract/Service IDs\n",
    "   - Contract dates\n",
    "   - Work order numbers\n",
    "   - Customer information\n",
    "3. Format the response in a clear, structured way\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Get response from Ollama\n",
    "        response = query_llm(prompt)\n",
    "        return response.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Response generation failed: {e}\")\n",
    "        return f\"Failed to generate response: {str(e)}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Kontrak mana yang akan berakhir pada tahun 2025?\n",
      "Retrieving relevant documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berikut adalah perhitungan kontrak yang akan berakhir pada tahun 2025:\n",
      "\n",
      "- Kita memulai dengan mengetahui id dan tanggal mulai dari kedua kontrak tersebut.\n",
      "- Tanggal kontrak awal untuk kedua kontrak adalah 1 Juli 2020 dan 30 April 2022. \n",
      "\n",
      "Untuk menentukan kontrak berakhir pada tahun 2025, kita perlu membandingkan tanggal akhir dari kedua kontrak tersebut.\n",
      "\n",
      "- Tanggal akhir dari kontrak ROUTER MIKROTIK CCR BANK IBK INDONESIA adalah 23 April 2025\n",
      "- Tanggal akhir dari kontrak MUF Bekasi adalah 30 September 2022\n",
      "\n",
      "Karena jumlah bulan yang berbeda antara kedua kontrak tersebut, maka kita harus membandingkan tahun kontrak untuk menentukan kontrak berakhir pada tahun 2025.\n",
      "\n",
      "- Tahun kontrak ROUTER MIKROTIK CCR BANK IBK INDONESIA adalah 2025\n",
      "- Tahun kontrak MUF Bekasi adalah 2022\n",
      "\n",
      "Karena tahun kontrak ROUTER MIKROTIK CCR BANK IBK INDONESIA adalah tahun 2025, maka kontrak tersebut akan berakhir pada tahun 2025.\n"
     ]
    }
   ],
   "source": [
    "query = \"Kontrak mana yang akan berakhir pada tahun 2025?\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"Retrieving relevant documents...\")\n",
    "response = generate_response(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
