{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import pickle\n",
    "import requests\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 14:27:36,723 - INFO - Loading dataset from file.\n",
      "2024-12-19 14:27:36,748 - INFO - Dataset loaded with 10 rows and 56 columns.\n"
     ]
    }
   ],
   "source": [
    "# Path to the JSONL file\n",
    "file_path = './data/test_data.jsonl'\n",
    "\n",
    "# Load and process the data\n",
    "logging.info(\"Loading dataset from file.\")\n",
    "data = pd.read_json(file_path, lines=True)\n",
    "logging.info(f\"Dataset loaded with {len(data)} rows and {len(data.columns)} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 14:27:36,773 - INFO - Initializing embedding model: all-MiniLM-L6-v2\n",
      "C:\\Users\\andidprastyo\\AppData\\Local\\Temp\\ipykernel_24216\\2649552398.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
      "c:\\Users\\andidprastyo\\Documents\\Github\\llm-query\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-19 14:27:43,423 - INFO - Use pytorch device_name: cpu\n",
      "2024-12-19 14:27:43,423 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Columns to embed\n",
    "columns_to_embed = [\n",
    "    'nama', 'produk', 'layanan', 'alamat', 'project_nama', 'no_wo', \n",
    "    'jenis_workorder', 'jenis_order', 'status_nodelink', \n",
    "    'customer', 'customer_direct', 'channeling', 'segmen', 'start_kontrak', 'end_kontrak'\n",
    "]\n",
    "\n",
    "# Combine columns into a single text field per row\n",
    "data['combined_text'] = data.apply(\n",
    "    lambda row: \" \\n \".join(\n",
    "        f\"{col}: {row[col]}\" for col in columns_to_embed if pd.notnull(row[col]) and row[col] != \"-\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "logging.info(f\"Initializing embedding model: {embedding_model_name}\")\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 14:27:46,857 - INFO - Creating vector stores for each column.\n",
      "2024-12-19 14:27:46,870 - INFO - Processing column: nama\n",
      "2024-12-19 14:27:46,988 - INFO - Loading faiss with AVX512 support.\n",
      "2024-12-19 14:27:46,988 - INFO - Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "2024-12-19 14:27:46,988 - INFO - Loading faiss with AVX2 support.\n",
      "2024-12-19 14:27:47,037 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2024-12-19 14:27:47,055 - INFO - Vector store created for column: nama, with 10 entries.\n",
      "2024-12-19 14:27:47,055 - INFO - Processing column: produk\n",
      "2024-12-19 14:27:47,121 - INFO - Vector store created for column: produk, with 10 entries.\n",
      "2024-12-19 14:27:47,122 - INFO - Processing column: layanan\n",
      "2024-12-19 14:27:47,174 - INFO - Vector store created for column: layanan, with 10 entries.\n",
      "2024-12-19 14:27:47,174 - INFO - Processing column: alamat\n",
      "2024-12-19 14:27:47,354 - INFO - Vector store created for column: alamat, with 9 entries.\n",
      "2024-12-19 14:27:47,354 - INFO - Processing column: project_nama\n",
      "2024-12-19 14:27:47,537 - INFO - Vector store created for column: project_nama, with 10 entries.\n",
      "2024-12-19 14:27:47,537 - INFO - Processing column: no_wo\n",
      "2024-12-19 14:27:47,623 - INFO - Vector store created for column: no_wo, with 10 entries.\n",
      "2024-12-19 14:27:47,623 - INFO - Processing column: jenis_workorder\n",
      "2024-12-19 14:27:47,692 - INFO - Vector store created for column: jenis_workorder, with 10 entries.\n",
      "2024-12-19 14:27:47,693 - INFO - Processing column: jenis_order\n",
      "2024-12-19 14:27:47,743 - INFO - Vector store created for column: jenis_order, with 10 entries.\n",
      "2024-12-19 14:27:47,743 - INFO - Processing column: status_nodelink\n",
      "2024-12-19 14:27:47,789 - INFO - Vector store created for column: status_nodelink, with 10 entries.\n",
      "2024-12-19 14:27:47,789 - INFO - Processing column: customer\n",
      "2024-12-19 14:27:47,861 - INFO - Vector store created for column: customer, with 10 entries.\n",
      "2024-12-19 14:27:47,863 - INFO - Processing column: customer_direct\n",
      "2024-12-19 14:27:47,914 - INFO - Vector store created for column: customer_direct, with 10 entries.\n",
      "2024-12-19 14:27:47,915 - INFO - Processing column: channeling\n",
      "2024-12-19 14:27:47,964 - INFO - Vector store created for column: channeling, with 10 entries.\n",
      "2024-12-19 14:27:47,964 - INFO - Processing column: segmen\n",
      "2024-12-19 14:27:48,032 - INFO - Vector store created for column: segmen, with 10 entries.\n",
      "2024-12-19 14:27:48,034 - INFO - Processing column: start_kontrak\n",
      "2024-12-19 14:27:48,105 - INFO - Vector store created for column: start_kontrak, with 10 entries.\n",
      "2024-12-19 14:27:48,105 - INFO - Processing column: end_kontrak\n",
      "2024-12-19 14:27:48,180 - INFO - Vector store created for column: end_kontrak, with 10 entries.\n",
      "2024-12-19 14:27:48,491 - INFO - Vector stores saved to 'vector_stores_by_column.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# Create separate vector stores for each column\n",
    "vector_stores = {}\n",
    "\n",
    "logging.info(\"Creating vector stores for each column.\")\n",
    "for column in columns_to_embed:\n",
    "    logging.info(f\"Processing column: {column}\")\n",
    "    # Drop rows with null or placeholder values for the current column\n",
    "    valid_rows = data[data[column].notnull() & (data[column] != \"-\")]\n",
    "    texts = valid_rows[column].tolist()\n",
    "    metadata = [{\"index\": idx, \"column\": column} for idx in valid_rows.index]\n",
    "    \n",
    "    # Create FAISS vector store for the column\n",
    "    vector_store = FAISS.from_texts(texts, embedding_model, metadatas=metadata)\n",
    "    vector_stores[column] = vector_store\n",
    "    logging.info(f\"Vector store created for column: {column}, with {len(texts)} entries.\")\n",
    "\n",
    "# Save the vector stores and the original data\n",
    "save_data = {\n",
    "    \"vector_stores\": vector_stores,\n",
    "    \"data\": data\n",
    "}\n",
    "\n",
    "vector_store_file = \"vector_stores_by_column.pkl\"\n",
    "with open(vector_store_file, \"wb\") as f:\n",
    "    pickle.dump(save_data, f)\n",
    "logging.info(f\"Vector stores saved to '{vector_store_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Querying Ollama, Context Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(prompt):\n",
    "    \"\"\"Query the Ollama API with a given prompt.\"\"\"\n",
    "    logging.info(\"Sending query to Ollama API.\")\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": \"llama3.2:1b\",\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.95,\n",
    "            \"max_tokens\": 150\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        if 'message' in result and 'content' in result['message']:\n",
    "            return result['message']['content'].strip()\n",
    "        else:\n",
    "            logging.warning(\"Unexpected response format from Ollama API.\")\n",
    "            return \"Error: Message or content not found in response.\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error querying Ollama API: {e}\")\n",
    "        return None\n",
    "\n",
    "def retrieve_context(query, vector_stores, target_column=None, data=None):\n",
    "    \"\"\"Retrieve relevant context from the FAISS vector store for a specific column.\"\"\"\n",
    "    if target_column not in vector_stores:\n",
    "        logging.error(f\"No vector store found for column '{target_column}'.\")\n",
    "        return None\n",
    "\n",
    "    # Extract exact value from query for no_wo\n",
    "    if target_column == 'no_wo':\n",
    "        # Extract the work order number from the query\n",
    "        wo_number = query.split(\": \")[-1].strip().strip(\"?\")\n",
    "        # Find exact matches in the data\n",
    "        exact_matches = data[data[target_column] == wo_number].index.tolist()\n",
    "        if exact_matches:\n",
    "            docs = []\n",
    "            for idx in exact_matches:\n",
    "                docs.append(type('Document', (), {\n",
    "                    'page_content': data.loc[idx, target_column],\n",
    "                    'metadata': {'index': idx, 'column': target_column}\n",
    "                })())\n",
    "            logging.info(f\"Found exact match for work order number: {wo_number}\")\n",
    "        else:\n",
    "            logging.info(f\"No exact match found for work order number: {wo_number}\")\n",
    "            docs = []\n",
    "    else:\n",
    "        # Use vector similarity for other columns\n",
    "        retriever = vector_stores[target_column].as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 5}\n",
    "        )\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Log retrieved documents\n",
    "    logging.info(f\"Retrieved {len(docs)} documents for column '{target_column}'.\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        logging.info(f\"Doc {i + 1}: Content: {doc.page_content}, Metadata: {doc.metadata}\")\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def ask_dataset(query, vector_stores, data, target_column):\n",
    "    \"\"\"Retrieve context and ask the dataset using the Ollama API.\"\"\"\n",
    "    # Retrieve relevant context\n",
    "    docs = retrieve_context(query, vector_stores, target_column, data)\n",
    "    if not docs:\n",
    "        return \"Error: No relevant context found for the query.\"\n",
    "\n",
    "    # Extract the original data entries\n",
    "    contexts = []\n",
    "    for doc in docs:\n",
    "        index = doc.metadata.get('index')\n",
    "        if index in data.index:\n",
    "            original_data = data.loc[index]\n",
    "            contexts.append({\n",
    "                'relevance_index': len(contexts) + 1,\n",
    "                'data': {col: original_data[col] for col in data.columns \n",
    "                        if pd.notnull(original_data[col]) and original_data[col] != \"-\"}\n",
    "            })\n",
    "\n",
    "    # Format context for the prompt\n",
    "    formatted_context = \"\\n\\n\".join([\n",
    "        f\"Result {ctx['relevance_index']}:\\n\" + \n",
    "        \"\\n\".join([f\"{k}: {v}\" for k, v in ctx['data'].items()])\n",
    "        for ctx in contexts\n",
    "    ])\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"Context:\n",
    "{formatted_context}\n",
    "\n",
    "Question: {query}\n",
    "Please analyze the above results and provide a concise answer focusing on the exact matching information.\"\"\"\n",
    "\n",
    "    # Query Ollama\n",
    "    response = query_ollama(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 14:27:48,937 - INFO - Found exact match for work order number: MI.0010/D1.200/MS.00/TSAT/05.2020\n",
      "2024-12-19 14:27:48,937 - INFO - Retrieved 1 documents for column 'no_wo'.\n",
      "2024-12-19 14:27:48,937 - INFO - Doc 1: Content: MI.0010/D1.200/MS.00/TSAT/05.2020, Metadata: {'index': 6, 'column': 'no_wo'}\n",
      "2024-12-19 14:27:48,937 - INFO - Sending query to Ollama API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from the dataset:\n",
      " Based on the provided result, the contract with a working order number of \"MI.0010/D1.200/MS.00/TSAT/05.2020\" is associated with:\n",
      "\n",
      "- Id: 571 (customer: SANATEL)\n",
      "- Customer Direct: 571 (customer: SANATEL)\n"
     ]
    }
   ],
   "source": [
    "# Load the saved vector stores\n",
    "with open(vector_store_file, \"rb\") as f:\n",
    "    saved_data = pickle.load(f)\n",
    "    vector_stores = saved_data[\"vector_stores\"]\n",
    "    data = saved_data[\"data\"]\n",
    "\n",
    "# Example query - can search by any column\n",
    "query = \"Which contracts has a working order number of : MI.0010/D1.200/MS.00/TSAT/05.2020?\"\n",
    "target_column = \"no_wo\"  # Can be any column from the data\n",
    "response = ask_dataset(query, vector_stores, data, target_column)\n",
    "print(\"Response from the dataset:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
