{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import pickle\n",
    "import requests\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 13:59:06,140 - INFO - Loading dataset from file.\n",
      "2024-12-20 13:59:06,216 - INFO - Dataset loaded with 10 rows and 56 columns.\n"
     ]
    }
   ],
   "source": [
    "# Path to the JSONL file\n",
    "file_path = './data/test_data.jsonl'\n",
    "\n",
    "# Load and process the data\n",
    "logging.info(\"Loading dataset from file.\")\n",
    "data = pd.read_json(file_path, lines=True)\n",
    "logging.info(f\"Dataset loaded with {len(data)} rows and {len(data.columns)} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 13:59:06,243 - INFO - Initializing embedding model: all-MiniLM-L6-v2\n",
      "C:\\Users\\andidprastyo\\AppData\\Local\\Temp\\ipykernel_3664\\137906283.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
      "c:\\Users\\andidprastyo\\Documents\\Github\\llm-query\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-20 13:59:16,528 - INFO - Use pytorch device_name: cpu\n",
      "2024-12-20 13:59:16,529 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Columns to embed\n",
    "columns_to_embed = [\n",
    "    'nama', 'sid', 'sid_tsat', 'produk', 'layanan', 'alamat', 'project_nama', 'no_wo', \n",
    "    'jenis_workorder', 'jenis_order', 'status_nodelink', \n",
    "    'customer', 'customer_direct', 'channeling', 'segmen', 'start_kontrak', 'end_kontrak'\n",
    "]\n",
    "\n",
    "# Combine columns into a single text field per row\n",
    "data['combined_text'] = data.apply(\n",
    "    lambda row: \" \\n \".join(\n",
    "        f\"{col}: {row[col]}\" for col in columns_to_embed if pd.notnull(row[col]) and row[col] != \"-\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "logging.info(f\"Initializing embedding model: {embedding_model_name}\")\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 13:59:20,002 - INFO - Creating vector stores for each column.\n",
      "2024-12-20 13:59:20,003 - INFO - Processing column: nama\n",
      "2024-12-20 13:59:20,253 - INFO - Loading faiss with AVX512 support.\n",
      "2024-12-20 13:59:20,253 - INFO - Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "2024-12-20 13:59:20,255 - INFO - Loading faiss with AVX2 support.\n",
      "2024-12-20 13:59:20,359 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2024-12-20 13:59:20,381 - INFO - Vector store created for column: nama, with 10 entries.\n",
      "2024-12-20 13:59:20,382 - INFO - Processing column: sid\n",
      "2024-12-20 13:59:20,429 - INFO - Vector store created for column: sid, with 10 entries.\n",
      "2024-12-20 13:59:20,430 - INFO - Processing column: sid_tsat\n",
      "2024-12-20 13:59:20,524 - INFO - Vector store created for column: sid_tsat, with 6 entries.\n",
      "2024-12-20 13:59:20,526 - INFO - Processing column: produk\n",
      "2024-12-20 13:59:20,573 - INFO - Vector store created for column: produk, with 10 entries.\n",
      "2024-12-20 13:59:20,574 - INFO - Processing column: layanan\n",
      "2024-12-20 13:59:20,663 - INFO - Vector store created for column: layanan, with 10 entries.\n",
      "2024-12-20 13:59:20,665 - INFO - Processing column: alamat\n",
      "2024-12-20 13:59:20,845 - INFO - Vector store created for column: alamat, with 9 entries.\n",
      "2024-12-20 13:59:20,846 - INFO - Processing column: project_nama\n",
      "2024-12-20 13:59:21,173 - INFO - Vector store created for column: project_nama, with 10 entries.\n",
      "2024-12-20 13:59:21,173 - INFO - Processing column: no_wo\n",
      "2024-12-20 13:59:21,251 - INFO - Vector store created for column: no_wo, with 10 entries.\n",
      "2024-12-20 13:59:21,253 - INFO - Processing column: jenis_workorder\n",
      "2024-12-20 13:59:21,301 - INFO - Vector store created for column: jenis_workorder, with 10 entries.\n",
      "2024-12-20 13:59:21,302 - INFO - Processing column: jenis_order\n",
      "2024-12-20 13:59:21,342 - INFO - Vector store created for column: jenis_order, with 10 entries.\n",
      "2024-12-20 13:59:21,343 - INFO - Processing column: status_nodelink\n",
      "2024-12-20 13:59:21,381 - INFO - Vector store created for column: status_nodelink, with 10 entries.\n",
      "2024-12-20 13:59:21,382 - INFO - Processing column: customer\n",
      "2024-12-20 13:59:21,444 - INFO - Vector store created for column: customer, with 10 entries.\n",
      "2024-12-20 13:59:21,444 - INFO - Processing column: customer_direct\n",
      "2024-12-20 13:59:21,489 - INFO - Vector store created for column: customer_direct, with 10 entries.\n",
      "2024-12-20 13:59:21,490 - INFO - Processing column: channeling\n",
      "2024-12-20 13:59:21,527 - INFO - Vector store created for column: channeling, with 10 entries.\n",
      "2024-12-20 13:59:21,528 - INFO - Processing column: segmen\n",
      "2024-12-20 13:59:21,569 - INFO - Vector store created for column: segmen, with 10 entries.\n",
      "2024-12-20 13:59:21,570 - INFO - Processing column: start_kontrak\n",
      "2024-12-20 13:59:21,612 - INFO - Vector store created for column: start_kontrak, with 10 entries.\n",
      "2024-12-20 13:59:21,614 - INFO - Processing column: end_kontrak\n",
      "2024-12-20 13:59:21,657 - INFO - Vector store created for column: end_kontrak, with 10 entries.\n",
      "2024-12-20 13:59:21,977 - INFO - Vector stores saved to 'vector_stores_by_column.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# Create separate vector stores for each column\n",
    "vector_stores = {}\n",
    "\n",
    "logging.info(\"Creating vector stores for each column.\")\n",
    "for column in columns_to_embed:\n",
    "    logging.info(f\"Processing column: {column}\")\n",
    "    # Drop rows with null or placeholder values for the current column\n",
    "    valid_rows = data[data[column].notnull() & (data[column] != \"-\")]\n",
    "    texts = valid_rows[column].tolist()\n",
    "    metadata = [{\"index\": idx, \"column\": column} for idx in valid_rows.index]\n",
    "    \n",
    "    # Create FAISS vector store for the column\n",
    "    # Convert all texts to strings to accommodate int values\n",
    "    texts = [str(text) for text in texts]\n",
    "    vector_store = FAISS.from_texts(texts, embedding_model, metadatas=metadata)\n",
    "    vector_stores[column] = vector_store\n",
    "    logging.info(f\"Vector store created for column: {column}, with {len(texts)} entries.\")\n",
    "\n",
    "# Save the vector stores and the original data\n",
    "save_data = {\n",
    "    \"vector_stores\": vector_stores,\n",
    "    \"data\": data\n",
    "}\n",
    "\n",
    "vector_store_file = \"vector_stores_by_column.pkl\"\n",
    "with open(vector_store_file, \"wb\") as f:\n",
    "    pickle.dump(save_data, f)\n",
    "logging.info(f\"Vector stores saved to '{vector_store_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Querying Ollama, Context Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(prompt):\n",
    "    \"\"\"Query the Ollama API with a given prompt.\"\"\"\n",
    "    logging.info(\"Sending query to Ollama API.\")\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": \"llama3.2:1b\",\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.95,\n",
    "            \"max_tokens\": 150\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        if 'message' in result and 'content' in result['message']:\n",
    "            return result['message']['content'].strip()\n",
    "        else:\n",
    "            logging.warning(\"Unexpected response format from Ollama API.\")\n",
    "            return \"Error: Message or content not found in response.\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error querying Ollama API: {e}\")\n",
    "        return None\n",
    "\n",
    "def retrieve_context(query, vector_stores, target_column=None, data=None):\n",
    "    \"\"\"Retrieve relevant context from the FAISS vector store for specific columns.\"\"\"\n",
    "    if target_column not in vector_stores:\n",
    "        logging.error(f\"No vector store found for column '{target_column}'.\")\n",
    "        return None\n",
    "\n",
    "    docs = []\n",
    "    \n",
    "    # Handle exact match columns (IDs and dates)\n",
    "    if target_column in ['sid', 'sid_tsat', 'no_wo', 'start_kontrak', 'end_kontrak']:\n",
    "        # Clean and extract the search value\n",
    "        if \":\" in query:\n",
    "            search_value = query.split(\":\")[-1].strip()\n",
    "        else:\n",
    "            # Extract the last part of the query\n",
    "            search_value = query.split()[-1].strip()\n",
    "        \n",
    "        # Remove any trailing punctuation or spaces\n",
    "        search_value = search_value.rstrip('?.,!')\n",
    "\n",
    "        exact_matches = []\n",
    "        \n",
    "        if target_column in ['start_kontrak', 'end_kontrak']:\n",
    "            # First try exact date match (YYYY-MM-DD)\n",
    "            if re.match(r'20\\d{2}-\\d{2}-\\d{2}', search_value):\n",
    "                exact_matches = data[data[target_column] == search_value].index.tolist()\n",
    "            \n",
    "            # Then try year match (YYYY)\n",
    "            elif re.match(r'20\\d{2}', search_value):\n",
    "                exact_matches = data[data[target_column].str.startswith(search_value, na=False)].index.tolist()\n",
    "            \n",
    "            # Handle relative year terms\n",
    "            else:\n",
    "                current_year = pd.Timestamp.now().year\n",
    "                relative_terms = {\n",
    "                    'this year': str(current_year),\n",
    "                    'next year': str(current_year + 1),\n",
    "                    'previous year': str(current_year - 1),\n",
    "                    'last year': str(current_year - 1)\n",
    "                }\n",
    "                \n",
    "                for term, year in relative_terms.items():\n",
    "                    if term.lower() in query.lower():\n",
    "                        exact_matches = data[data[target_column].str.startswith(year, na=False)].index.tolist()\n",
    "                        break\n",
    "                \n",
    "        elif target_column in ['sid', 'sid_tsat']:\n",
    "            try:\n",
    "                # Try numeric conversion\n",
    "                numeric_value = pd.to_numeric(search_value)\n",
    "                exact_matches = data[data[target_column] == numeric_value].index.tolist()\n",
    "            except ValueError:\n",
    "                # Fallback to string comparison\n",
    "                exact_matches = data[data[target_column].astype(str) == search_value].index.tolist()\n",
    "        \n",
    "        else:  # no_wo\n",
    "            # Use string comparison\n",
    "            exact_matches = data[data[target_column].astype(str) == search_value].index.tolist()\n",
    "        \n",
    "        if exact_matches:\n",
    "            for idx in exact_matches:\n",
    "                docs.append(type('Document', (), {\n",
    "                    'page_content': str(data.loc[idx, target_column]),\n",
    "                    'metadata': {'index': idx, 'column': target_column}\n",
    "                })())\n",
    "            logging.info(f\"Found exact match(es) for {target_column}: {search_value}\")\n",
    "        else:\n",
    "            logging.info(f\"No exact match found for {target_column}: {search_value}\")\n",
    "            \n",
    "            # Only if no exact matches are found, use vector similarity as fallback\n",
    "            retriever = vector_stores[target_column].as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={\"k\": 5}\n",
    "            )\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            \n",
    "            if docs:\n",
    "                logging.info(f\"Using similarity search as fallback for {target_column}\")\n",
    "\n",
    "    # Remove duplicates while preserving order\n",
    "    seen_indices = set()\n",
    "    unique_docs = []\n",
    "    for doc in docs:\n",
    "        idx = doc.metadata['index']\n",
    "        if idx not in seen_indices:\n",
    "            seen_indices.add(idx)\n",
    "            unique_docs.append(doc)\n",
    "    docs = unique_docs[:10]\n",
    "\n",
    "    # Log retrieved documents\n",
    "    logging.info(f\"Retrieved {len(docs)} documents for column '{target_column}'.\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        logging.info(f\"Doc {i + 1}: Content: {doc.page_content}, Metadata: {doc.metadata}\")\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def ask_dataset(query, vector_stores, data, target_column):\n",
    "    docs = retrieve_context(query, vector_stores, target_column, data)\n",
    "    if not docs:\n",
    "        return \"Error: No relevant context found for the query.\"\n",
    "\n",
    "    contexts = []\n",
    "    for doc in docs:\n",
    "        index = doc.metadata.get('index')\n",
    "        if index in data.index:\n",
    "            original_data = data.loc[index]\n",
    "            contexts.append({\n",
    "                'relevance_index': len(contexts) + 1,\n",
    "                'data': {col: original_data[col] for col in data.columns \n",
    "                        if pd.notnull(original_data[col]) and original_data[col] != \"-\"}\n",
    "            })\n",
    "\n",
    "    formatted_context = \"\\n\\n\".join([\n",
    "        f\"Result {ctx['relevance_index']}:\\n\" + \n",
    "        \"\\n\".join([f\"{k}: {v}\" for k, v in ctx['data'].items()])\n",
    "        for ctx in contexts\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"Context:\n",
    "{formatted_context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the results and prioritize contracts ending in (years if years else the specified year).\n",
    "2. Use exact year matches over similarity ones.\n",
    "3. Include and verify all relevant date information.\n",
    "4. Preserve data formatting in your response.\n",
    "5. Provide clear conclusions based on the extracted data.\n",
    "\n",
    "Provide a concise answer based on this context.\"\"\"\n",
    "\n",
    "    response = query_ollama(prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 13:59:22,358 - INFO - Found exact match(es) for sid: 5480001690001\n",
      "2024-12-20 13:59:22,360 - INFO - Retrieved 1 documents for column 'sid'.\n",
      "2024-12-20 13:59:22,361 - INFO - Doc 1: Content: 5480001690001, Metadata: {'index': 8, 'column': 'sid'}\n",
      "2024-12-20 13:59:22,366 - INFO - Sending query to Ollama API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from the dataset:\n",
      " Berikut adalah hasil pencarian untuk sid: 5480001690001:\n",
      "\n",
      "- Start kontrak: 2020\n",
      "- End kontrak: 2022\n",
      "- Jenis workorder: Aktivasi\n",
      "- Status nodelink: Operasional\n",
      "- Customer Direct: TELKOM DES\n",
      "- Channeling: INDIRECT DES\n",
      "- Segmen: BANKING\n",
      "\n",
      "Dari hasil pencarian, ditemukan satu kontrak yang memenuhi kondisi yang ditanyakan.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved vector stores\n",
    "with open(vector_store_file, \"rb\") as f:\n",
    "    saved_data = pickle.load(f)\n",
    "    vector_stores = saved_data[\"vector_stores\"]\n",
    "    data = saved_data[\"data\"]\n",
    "\n",
    "# Example query - can search by any column\n",
    "query = \"Which contracts has sid : 5480001690001\"\n",
    "target_column = \"sid\"  # Can be any column from the data\n",
    "response = ask_dataset(query, vector_stores, data, target_column)\n",
    "print(\"Response from the dataset:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 13:59:52,129 - INFO - Found exact match(es) for end_kontrak: 2025\n",
      "2024-12-20 13:59:52,130 - INFO - Retrieved 2 documents for column 'end_kontrak'.\n",
      "2024-12-20 13:59:52,132 - INFO - Doc 1: Content: 2025-04-23, Metadata: {'index': 1, 'column': 'end_kontrak'}\n",
      "2024-12-20 13:59:52,133 - INFO - Doc 2: Content: 2025-04-23, Metadata: {'index': 2, 'column': 'end_kontrak'}\n",
      "2024-12-20 13:59:52,138 - INFO - Sending query to Ollama API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from the dataset:\n",
      " Based on the provided results, the contracts that end in 2025 are:\n",
      "\n",
      "- Result 1: id: 6000, nama: MANAGE SERVICE RADIO IP BANK IBK INDONESIA\n",
      "- Result 2: id: 16632, nama: ROUTER MIKROTIK CCR BANK IBK INDONESIA\n",
      "\n",
      "These two contracts end in the year 2025.\n"
     ]
    }
   ],
   "source": [
    "query = \"Which contracts end in 2025\"\n",
    "target_column = \"end_kontrak\"  # Can be any column from the data\n",
    "response = ask_dataset(query, vector_stores, data, target_column)\n",
    "print(\"Response from the dataset:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
